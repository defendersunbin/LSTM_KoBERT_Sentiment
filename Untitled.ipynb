{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a30a31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8e337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Downloading flask-2.3.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 0.0/96.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 96.1/96.1 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\ahn\\anaconda3\\envs\\capstonstockproject\\lib\\site-packages (from Flask) (8.1.5)\n",
      "Collecting Werkzeug>=2.3.7\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "     ---------------------------------------- 0.0/242.2 kB ? eta -:--:--\n",
      "     ----------- --------------------------- 71.7/242.2 kB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ---------- 174.1/242.2 kB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 242.2/242.2 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting blinker>=1.6.2\n",
      "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in c:\\users\\ahn\\anaconda3\\envs\\capstonstockproject\\lib\\site-packages (from Flask) (6.6.0)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\ahn\\anaconda3\\envs\\capstonstockproject\\lib\\site-packages (from Flask) (3.1.2)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahn\\anaconda3\\envs\\capstonstockproject\\lib\\site-packages (from click>=8.1.3->Flask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ahn\\anaconda3\\envs\\capstonstockproject\\lib\\site-packages (from importlib-metadata>=3.6.0->Flask) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ahn\\anaconda3\\envs\\capstonstockproject\\lib\\site-packages (from Jinja2>=3.1.2->Flask) (2.1.2)\n",
      "Installing collected packages: Werkzeug, itsdangerous, blinker, Flask\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.3.4\n",
      "    Uninstalling Werkzeug-2.3.4:\n",
      "      Successfully uninstalled Werkzeug-2.3.4\n",
      "Successfully installed Flask-2.3.3 Werkzeug-2.3.7 blinker-1.6.2 itsdangerous-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script flask.exe is installed in 'C:\\Users\\ahn\\anaconda3\\envs\\CapstonStockProject\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36414bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 키워드 입력 : 삼성전자\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 138/138 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[뉴스 제목]\n",
      "['삼성전자, 개발자 콘퍼런스 개최…한종희, 비전 발표', '\"삼성전자 3분기 영업익 2조1천억원\"', \"삼성전자, 'IFA 2023'서 세탁기에서 건조까지 가능한 신제품 공개\", \"삼성전자, '속도·안정성' 잡은 SD카드 신제품 출시\", '경계현 삼성전자 사장 \"소부장 업체와 \\'윈윈\\'…성장 위한 열쇠\"', \"삼성전자, '삼성 개발자 콘퍼런스 2023' 10월 개최\", \"삼성전자, IFA 2023서 '세탁·건조 올인원' 제품 공개\", '삼박자 맞아 떨어지자… 동학개미, 8개월 만에 삼성전자 주주로 돌아왔다', '중기부·삼성전자, 유망 팹리스 스타트업 지원', \"이달만 삼성전자 1조 넘게 팔더니…기관 '이 종목'은 담았다\", '삼성전자, 男 직원 육아휴직 1000명 돌파…LG·SK도 증가', '“영화 1편 11.2초에”…삼성전자, 메모리카드 ‘프로 얼티밋’ 출시', '메모리 반등 ‘열쇠’, 낸드플래시…삼성전자 신규시장서 일 낸다', '중기부·삼성전자, 다모아텍 등 5곳 유망 팹리스 선정해 지원(종합)', '산업부, 삼성전자·SK하이닉스 등과 반도체 패키징 R&amp;D MOU', '[New &amp; Good] \"액션캠·드론에 최적화\" 삼성전자, 새 메모리카드 선 봬', '중기부 · 삼성전자, 유망 팹리스 5곳에 지원 강화', '삼성전자, 메모리카드 ‘프로 얼티밋’ 출시…속도·안정성 강화', '“영화 1편 11.2초에”…삼성전자, 고해상도 콘텐츠용 SD카드 ‘프로 얼티밋’ 선보여', \"삼성전자, 평택 P1 낸드 생산라인 '128단 → 236단' 전환[숏잇슈]\", \"중기부·삼성전자 '팹리스 챌린지', AI·모빌리티 등으로 확대\", '대기업 자녀세대 주식자산 비중 50% 이상 22곳…10년새 10개↑', '삼성 준감위 “수직적 지배구조 해법 아직 못 찾아”', '폴더블 시장도 맹추격하는 中...삼성도 위협', '삼성도 내놨다…일체형 세탁·건조기 IFA서 공개', '삼성·LG 가전 첫 협업 \"스마트홈 앱 상호연동\"', '‘삼성 개발자 콘퍼런스’ 10월 美개최', '\"삼성·현대차·LG 등 22곳은 승계 작업 완료\"', \"자립준비청년 '경제 자립' 돕는다…'삼성희망디딤돌 2.0' 출범\", '대기업 자녀세대, 주식자산 비중 또 늘었다', '‘삼성 개발자 콘퍼런스’ 10월 5일 美서 개최', '삼성 스마트싱스로 LG 가전 제어한다', '삼성 ‘디딤돌 2.0’… “보호시설 퇴소 청년에 취업교육”', '삼성, 청년 경제자립 돕는다... 취업 교육 ‘희망디딤돌 2.0′ 출범', \"'나이됐으니 자립하라는 가혹'…삼성, 보호종료 청소년 지원 이유\", '\"영화 1편 11초만에\" 삼성 SD카드 신제품', \"이재용 '미래 동행'…삼성, 자립준비청년 '경제적 자립' 돕는다\", '‘라이벌’ 삼성·LG, 스마트홈 확장 위해 손잡았다', '애플, 상반기 스마트폰 출하량 1~4위 싹쓸이… 삼성은 5위', '\"이젠 반도체 기술자 꿈꿔요\"…자립준비 청소년 지원 팔걷은 삼성', '삼성 ‘스마트싱스’, LG ‘씽큐’ 연내 상호 연동… 양사 가전 제어한다', '삼성, 25개 협력사에 \\'눈높이 컨설팅\\'…경계현 사장 \"상생관계 구축, 성공에 필수적\"', '\"게임 인구 37억명 시대\"…삼성·LG, 프리미엄 모니터 앞세워 정조준', \"삼성, 평택 P1 낸드 '236단' 전환…128단 생산 줄여\", '이찬희 삼성 준법감시위원장 \"지배구조 개선 해법 아직 못찾아\"', \"'LG 패널 단 삼성 TV' 유럽도 상륙…'83형 올레드' 영토 확장\", '삼성·LG 앱으로 다른 회사 가전도 제어…스마트홈 생태계 확장', \"자립준비청년 경제적 자립 돕는다...'삼성희망디딤돌 2.0' 출범\"]\n",
      "\n",
      "[뉴스 링크]\n",
      "['https://n.news.naver.com/mnews/article/003/0012058179?sid=101', 'https://n.news.naver.com/mnews/article/215/0001122103?sid=101', 'https://n.news.naver.com/mnews/article/001/0014159606?sid=101', 'https://n.news.naver.com/mnews/article/666/0000019994?sid=101', 'https://n.news.naver.com/mnews/article/421/0007017940?sid=101', 'https://n.news.naver.com/mnews/article/014/0005064240?sid=105', 'https://n.news.naver.com/mnews/article/629/0000235580?sid=105', 'https://n.news.naver.com/mnews/article/366/0000927929?sid=101', 'https://n.news.naver.com/mnews/article/021/0002590991?sid=101', 'https://n.news.naver.com/mnews/article/008/0004931482?sid=101', 'https://n.news.naver.com/mnews/article/003/0012058713?sid=101', 'https://n.news.naver.com/mnews/article/009/0005178962?sid=101', 'https://n.news.naver.com/mnews/article/003/0012058005?sid=101', 'https://n.news.naver.com/mnews/article/277/0005306795?sid=101', 'https://n.news.naver.com/mnews/article/448/0000425222?sid=101', 'https://n.news.naver.com/mnews/article/469/0000757752?sid=101', 'https://n.news.naver.com/mnews/article/055/0001085457?sid=101', 'https://n.news.naver.com/mnews/article/468/0000974982?sid=101', 'https://n.news.naver.com/mnews/article/024/0000084242?sid=101', 'https://n.news.naver.com/mnews/article/030/0003130436?sid=105', 'https://n.news.naver.com/mnews/article/421/0007018071?sid=101', 'https://n.news.naver.com/mnews/article/001/0014159530?sid=101', 'https://n.news.naver.com/mnews/article/366/0000927689?sid=101', 'https://n.news.naver.com/mnews/article/014/0005064225?sid=101', 'https://n.news.naver.com/mnews/article/018/0005562372?sid=101', 'https://n.news.naver.com/mnews/article/009/0005179226?sid=105', 'https://n.news.naver.com/mnews/article/016/0002190298?sid=105', 'https://n.news.naver.com/mnews/article/050/0000067927?sid=101', 'https://n.news.naver.com/mnews/article/001/0014158694?sid=102', 'https://n.news.naver.com/mnews/article/215/0001122075?sid=101', 'https://n.news.naver.com/mnews/article/021/0002591150?sid=101', 'https://n.news.naver.com/mnews/article/241/0003297407?sid=105', 'https://n.news.naver.com/mnews/article/020/0003517777?sid=101', 'https://n.news.naver.com/mnews/article/366/0000927805?sid=105', 'https://n.news.naver.com/mnews/article/648/0000019087?sid=101', 'https://n.news.naver.com/mnews/article/009/0005179184?sid=101', 'https://n.news.naver.com/mnews/article/629/0000235482?sid=101', 'https://n.news.naver.com/mnews/article/018/0005561919?sid=101', 'https://n.news.naver.com/mnews/article/417/0000945263?sid=105', 'https://n.news.naver.com/mnews/article/018/0005562080?sid=101', 'https://n.news.naver.com/mnews/article/005/0001634281?sid=101', 'https://n.news.naver.com/mnews/article/108/0003177989?sid=004', 'https://n.news.naver.com/mnews/article/031/0000769131?sid=101', 'https://n.news.naver.com/mnews/article/030/0003130368?sid=105', 'https://n.news.naver.com/mnews/article/417/0000945172?sid=101', 'https://n.news.naver.com/mnews/article/421/0007017624?sid=101', 'https://n.news.naver.com/mnews/article/011/0004231512?sid=101', 'https://n.news.naver.com/mnews/article/119/0002744030?sid=101']\n",
      "\n",
      "[뉴스 내용]\n",
      "['[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]', '[]']\n",
      "\n",
      "[뉴스 날짜]\n",
      "['2023-08-30 09:21:11', '2023-08-30 08:33:10', '2023-08-30 08:15:34', '2023-08-29 18:53:01', '2023-08-30 11:12:24', '2023-08-30 08:08:06', '2023-08-30 10:01:02', '2023-08-30 10:29:01', '2023-08-29 15:25:01', '2023-08-30 05:30:00', '2023-08-30 11:28:45', '2023-08-29 14:48:00', '2023-08-30 06:41:07', '2023-08-30 08:53:36', '2023-08-29 14:16:33', '2023-08-30 10:01:05', '2023-08-29 14:37:19', '2023-08-29 16:07:02', '2023-08-30 09:09:01', '2023-08-29 15:28:01', '2023-08-30 11:46:16', '2023-08-30 06:01:01', '2023-08-29 14:21:01', '2023-08-30 06:46:04', '2023-08-30 08:40:05', '2023-08-29 17:57:01', '2023-08-30 11:16:04', '2023-08-30 12:39:01', '2023-08-29 15:30:00', '2023-08-30 06:39:24', '2023-08-30 11:49:19', '2023-08-29 16:20:12', '2023-08-30 03:05:05', '2023-08-29 17:30:01', '2023-08-29 17:33:01', '2023-08-29 17:49:11', '2023-08-29 15:30:04', '2023-08-29 13:54:05', '2023-08-30 05:50:00', '2023-08-29 16:18:07', '2023-08-29 15:47:01', '2023-08-30 12:38:54', '2023-08-30 09:10:01', '2023-08-29 13:42:01', '2023-08-29 15:14:18', '2023-08-30 09:37:40', '2023-08-29 14:17:01', '2023-08-29 15:30:02']\n",
      "news_title:  48\n",
      "news_url:  48\n",
      "news_contents:  48\n",
      "news_dates:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 뉴스에 대한 평균 감성 점수 값: 0.48999999999999994\n",
      "주가의 하락 예측\n",
      "긍정 뉴스의 개수: 25\n",
      "부정 뉴스의 개수: 23\n",
      "주가 상승으로 예측\n",
      "48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['삼성', '삼성전자', '지원', '개발자', '가전', '자립', 'IFA', '2023', '신제품', '공개']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 크롤링시 필요한 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 페이지 입력 (1 페이지당 기사 10개 이하)\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num + 1\n",
    "    else:\n",
    "        return num + 9 * (num - 1)\n",
    "\n",
    "\n",
    "# search : 검색어, pd=4 : 최근 1일, start_page : 몇 페이지\n",
    "def makeUrl(search, start_pg, end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        # 정확도순(디폴트)으로 1일간의 뉴스(pd=4) \n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search +\"&start=\" + str(\n",
    "            start_page)\n",
    "        \n",
    "        return url\n",
    "    else:\n",
    "        # url 부분에서 정확도순서로 1일 데이터를 분류 가능\n",
    "        urls = []\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "            page = makePgNum(i)\n",
    "            \n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search +\"&pd=4\"+\"&start=\" + str(page)\n",
    "            #url = \"https://search.naver.com/search.naver?where=news&query=%EC%B9%B4%EC%B9%B4%EC%98%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2021.09.10&de=2021.09.15&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20210910to20210915&is_sug_officeid=0\"+\"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        return urls\n",
    "\n",
    "    # html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "\n",
    "# 기사 내용 크롤링 함수\n",
    "def news_attrs_crawler(articles, attrs):\n",
    "    attrs_content = []\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "# html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    # html 불러오기\n",
    "    original_html = requests.get(i, headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\n",
    "        \"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver, 'href')\n",
    "    return url\n",
    "\n",
    "#####뉴스크롤링 시작#####\n",
    "\n",
    "# 검색어 입력\n",
    "search = input(\"검색 키워드 입력 : \")\n",
    "# 검색 시작할 페이지 입력\n",
    "page = 1\n",
    "# 검색 종료할 페이지 입력\n",
    "page2 = 10\n",
    "\n",
    "searches = ['삼성전자','LG에너지솔루션','SK하이닉스','삼성바이오로직스','POSCO홀딩스',\"LG화학\",\"삼성SDI\",\"현대차\",\"NAVER\"]\n",
    "\n",
    "# naver url 생성\n",
    "url = makeUrl(search, page, page2)\n",
    "\n",
    "# 뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url = []\n",
    "news_contents = []\n",
    "news_dates = []\n",
    "\n",
    "for i in url:\n",
    "    url = articles_crawler(url)\n",
    "    news_url.append(url)\n",
    "\n",
    "\n",
    "# 제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "\n",
    "# 제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "# 1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1, news_url)\n",
    "\n",
    "# NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 뉴스 내용 크롤링\n",
    "for i in tqdm(final_urls):\n",
    "    # 각 기사 html get하기\n",
    "    news = requests.get(i, headers=headers)\n",
    "    news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "    # 뉴스 본문 가져오기 (일단 구현은 해놓음 but 일단 기사 제목 수준에서 진행)\n",
    "    content = news_html.select(\"div#dic_area\")\n",
    "    if content == []:\n",
    "        content = news_html.select(\"#articeBody\")\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "    content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2, '')\n",
    "    \n",
    "    news_titles.append(title)\n",
    "    news_contents.append(content)\n",
    "\n",
    "    try:\n",
    "        html_date = news_html.select_one(\n",
    "            \"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "        news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "        news_date = re.sub(pattern=pattern1, repl='', string=str(news_date))\n",
    "        news_date = news_date[0:10]\n",
    "    # 날짜 가져오기\n",
    "    news_dates.append(news_date)\n",
    "\n",
    "print(\"\\n[뉴스 제목]\")\n",
    "print(news_titles)\n",
    "print(\"\\n[뉴스 링크]\")\n",
    "print(final_urls)\n",
    "print(\"\\n[뉴스 내용]\")\n",
    "print(news_contents)\n",
    "print(\"\\n[뉴스 날짜]\")\n",
    "print(news_dates)\n",
    "\n",
    "print('news_title: ', len(news_titles))\n",
    "print('news_url: ', len(final_urls))\n",
    "print('news_contents: ', len(news_contents))\n",
    "print('news_dates: ', len(news_dates))\n",
    "\n",
    "news_df = pd.DataFrame({'date': news_dates, 'title': news_titles, 'content' : news_contents})\n",
    "news_df\n",
    "\n",
    "from konlpy.tag import Hannanum  # Hannanum 형태소 분석기 불러오기\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "df = news_df\n",
    "\n",
    "# KoELECTRA 모델 로드\n",
    "model_name = \"hyunwoongko/kobart\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 형태소 분석 함수\n",
    "hannanum = Hannanum()  # Hannanum 형태소 분석기 객체 생성\n",
    "def tokenize(text):\n",
    "    return hannanum.morphs(text)\n",
    "\n",
    "# title 열에 대해 형태소 분석 적용\n",
    "\n",
    "df['title'] = df['title'].astype(str).apply(tokenize)\n",
    "# for i in range(df.shape[0]):\n",
    "#     df.iloc[i,1] = df.iloc[i,1].apply(tokenize)\n",
    "\n",
    "\n",
    "# 감성 분석을 위한 전처리 함수\n",
    "def preprocess(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    return inputs\n",
    "\n",
    "# 예측 함수\n",
    "def predict(inputs):\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    return probs[0].detach().cpu().numpy()\n",
    "\n",
    "# title을 예측해서 수치화시켜 sentiment으로 저장. 즉, title를 수치화 시킨 것이 sentiment\n",
    "df['sentiment'] = df['title'].apply(lambda x: predict(preprocess(' '.join(x))))\n",
    "\n",
    "# 0.5 기준으로 하면 부정확하긴 함... 정확도를 높이려면 이 부분 건들면 좋을 듯 또는 중립을 포함시키는 것도 해봐야 할 듯\n",
    "def convert_sentiment(probs):\n",
    "    if probs[0] < 0.5:\n",
    "        return 0\n",
    "    elif probs[0]>= 0.5:\n",
    "        return 1\n",
    "#     else:\n",
    "#         return '중립'\n",
    "\n",
    "# train할 label은 제목을 읽고 내가 직접 라벨링\n",
    "# test할 label은 0.5를 기준으로 sentiment가 0.5보다 크면 1, 작으면 0으로 기준 세움\n",
    "df['label'] = df['sentiment'].apply(convert_sentiment)\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: x[0]).tolist()\n",
    "df=round(df,2)\n",
    "\n",
    "senti_avg = df['sentiment'].mean()\n",
    "\n",
    "print(f\"모든 뉴스에 대한 평균 감성 점수 값: {df['sentiment'].mean()}\")\n",
    "if df['sentiment'].mean() >= 0.5:\n",
    "    print(\"주가의 상승 예측\")\n",
    "else:\n",
    "    print(\"주가의 하락 예측\")\n",
    "    \n",
    "sentiment = 0\n",
    "print(f\"긍정 뉴스의 개수: {df[df['label'] == 1].label.count()}\")\n",
    "print(f\"부정 뉴스의 개수: {df[df['label'] == 0].label.count()}\")\n",
    "\n",
    "if df[df['label'] == 1].label.count() > df[df['label'] == 0].label.count():\n",
    "    sentiment = 1\n",
    "    print(\"주가 상승으로 예측\")\n",
    "else:\n",
    "    sentiment = 0\n",
    "    print(\"주가 하락으로 예측\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "sentimented_title = []\n",
    "print(len(df['title']))\n",
    "for i in range(len(df['title'])):\n",
    "    for j in range(len(df['title'][i])):\n",
    "        if len(df['title'][i][j]) != 1: \n",
    "            sentimented_title.append(df['title'][i][j])\n",
    "\n",
    "counter = Counter(sentimented_title)\n",
    "counter\n",
    "\n",
    "keywords = []\n",
    "for i in range(10):\n",
    "    keywords.append(counter.most_common(10)[i][0])\n",
    "    \n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b87a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import joblib\n",
    "\n",
    "\n",
    "# 스태킹 모델에 사용할 알고리즘 - KNN, 랜덤포레스트, AdaBoost, 의사결정나무, 로지스틱회귀\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/analysis')\n",
    "def hello():\n",
    "    return \"hello WORLD!\"\n",
    "\n",
    "@app.route(\"/analysis\", methods = ['GET'])\n",
    "def predict():\n",
    "    if request.method == 'GET':\n",
    "        # 개별 ML 모델 객체 생성 (기반모델)\n",
    "        knn_clf = KNeighborsClassifier()\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        dt_clf = DecisionTreeClassifier()\n",
    "        ada_clf = AdaBoostClassifier()\n",
    "\n",
    "        # 메타모델(스태킹으로 만들어진 데이터 학습 및 예측)\n",
    "        lr_final = LogisticRegression(C=10)\n",
    "\n",
    "        appid = \"dd067a9aa888d5941ce0662190c0a2f4\"\n",
    "        geo = {\"함양\": [35.52, 127.75], \"서울\": [37.6387, 127.0370], \"인제\": [38.06, 128.17], \"합천\": [35.50, 128.16],\n",
    "               \"제천\": [37.13, 128.19], \"울산\": [35.53, 129.31], \"부산\": [35.17, 129.07], \"강진\": [34.80, 126.69],\n",
    "               \"양평\": [37.21, 127.18], \"광주\": [35.15, 126.85], \"금산\": [36.03, 127.48], \"태백\": [37.16, 128.98],\n",
    "               \"충주\": [36.99, 127.92], \"대구\": [35.87, 128.60], \"파주\": [37.75, 126.78], \"수원\": [37.26, 127.02]}\n",
    "        url = \"http://api.openweathermap.org/data/2.5/forecast?lat=\" + str(geo[\"서울\"][0]) + \"&lon=\" + str(\n",
    "            geo[\"서울\"][1]) + \"&appid=\" + appid + \"&units=metric\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        json_data = response.json()\n",
    "        s1 = json.dumps(json_data)\n",
    "        json_object = json.loads(s1)\n",
    "        forecast_list = json_object['list']\n",
    "\n",
    "        days = []\n",
    "\n",
    "        KST = timezone(timedelta(hours=9))\n",
    "        now = datetime.now(KST)\n",
    "        days.append(now.strftime('%Y-%m-%d'))\n",
    "        for i in range(1, 5):\n",
    "            day = now + timedelta(days=i)\n",
    "            days.append(day.strftime('%Y-%m-%d'))\n",
    "        midnight = \"00:00:00\"\n",
    "        sunrise = \"06:00:00\"\n",
    "\n",
    "        df = pd.DataFrame(index=[1])\n",
    "        pre_temp = 0\n",
    "\n",
    "        for forecast in forecast_list:\n",
    "\n",
    "            # API 결과 값에서 나오는 시간은 UTC(영국) 기준이기 때문에 9시간을 더해줘야 함\n",
    "            datetime_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "            datetime_string = forecast['dt_txt']\n",
    "            datetime_result = datetime.strptime(datetime_string, datetime_format)\n",
    "            datetime_result += timedelta(hours=9)\n",
    "            day = datetime_result.strftime('%Y-%m-%d')\n",
    "            dayTime = datetime_result.strftime('%H:%M:%S')\n",
    "\n",
    "            if day == days[1]:  # 임시로\n",
    "                if dayTime == midnight:\n",
    "                    newColumns = ['Previous Temperature', 'Previous Precipitation', 'Previous Humidity',\n",
    "                                  'Previous Pressure']\n",
    "                    pre_temp = float(forecast['main']['temp'])\n",
    "                    rain = 0\n",
    "                    humidity = forecast['main']['humidity']\n",
    "                    pressure = forecast['main']['pressure']\n",
    "                    if forecast['weather'][0]['main'] == 'Rain':\n",
    "                        rain = forecast['rain']['3h']\n",
    "                    df[newColumns] = pd.Series([pre_temp, rain, humidity, pressure])\n",
    "                    continue\n",
    "                if dayTime == sunrise:\n",
    "                    newColumns = ['Sunrise Temperature', 'Sunrise Wind Speed', 'Sunrise Humidity', 'Sunrise Pressure',\n",
    "                                  'Sunrise Cloud Coverage', 'Diurnal Temperature Range']\n",
    "                    temp = forecast['main']['temp']\n",
    "                    wind = forecast['wind']['speed']\n",
    "                    humidity = forecast['main']['humidity']\n",
    "                    pressure = forecast['main']['pressure']\n",
    "                    clouds = float(forecast['clouds']['all']) / 10\n",
    "                    temp_range = float(temp) - pre_temp\n",
    "                    df[newColumns] = pd.Series([temp, wind, humidity, pressure, clouds, temp_range])\n",
    "                    continue\n",
    "\n",
    "        knn_clf = joblib.load(\"/Users/ihyeonho/Desktop/workspace/flaskProjects/SEEcloudSEA/knn.pkl\")\n",
    "        rf_clf = joblib.load(\"/Users/ihyeonho/Desktop/workspace/flaskProjects/SEEcloudSEA/rf.pkl\")\n",
    "        dt_clf = joblib.load(\"/Users/ihyeonho/Desktop/workspace/flaskProjects/SEEcloudSEA/dt.pkl\")\n",
    "        ada_clf = joblib.load(\"/Users/ihyeonho/Desktop/workspace/flaskProjects/SEEcloudSEA/ada.pkl\")\n",
    "        lr_clf = joblib.load(\"/Users/ihyeonho/Desktop/workspace/flaskProjects/SEEcloudSEA/lr.pkl\")\n",
    "        xgbc_final = joblib.load(\"/Users/ihyeonho/Desktop/workspace/flaskProjects/SEEcloudSEA/xgbc.pkl\")\n",
    "\n",
    "        knn = knn_clf.predict(df)\n",
    "        rf = rf_clf.predict(df)\n",
    "        dt = dt_clf.predict(df)\n",
    "        ada = ada_clf.predict(df)\n",
    "        lr = lr_clf.predict(df)\n",
    "\n",
    "        stacked = np.array([knn, rf, dt, ada, lr])\n",
    "        stacked = np.transpose(stacked)\n",
    "\n",
    "        final_pred = xgbc_final.predict(stacked)\n",
    "\n",
    "        print(final_pred[0])\n",
    "        predict_result = str(final_pred[0])\n",
    "\n",
    "    return jsonify(predict_result), 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac84bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello WORLD!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb57191",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Working outside of request context.\n\nThis typically means that you attempted to use functionality that needed\nan active HTTP request. Consult the documentation on testing for\ninformation about how to avoid this problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mroute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m, methods \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m():\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m# 개별 ML 모델 객체 생성 (기반모델)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         knn_clf \u001b[38;5;241m=\u001b[39m KNeighborsClassifier()\n\u001b[0;32m     28\u001b[0m         rf_clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CapstonStockProject\\lib\\site-packages\\werkzeug\\local.py:311\u001b[0m, in \u001b[0;36m_ProxyLookup.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_current_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfallback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CapstonStockProject\\lib\\site-packages\\werkzeug\\local.py:508\u001b[0m, in \u001b[0;36mLocalProxy.__init__.<locals>._get_current_object\u001b[1;34m()\u001b[0m\n\u001b[0;32m    506\u001b[0m     obj \u001b[38;5;241m=\u001b[39m local\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(unbound_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_name(obj)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Working outside of request context.\n\nThis typically means that you attempted to use functionality that needed\nan active HTTP request. Consult the documentation on testing for\ninformation about how to avoid this problem."
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20320ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CapstonStockProject",
   "language": "python",
   "name": "capstonestockproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
